{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Natural Language Processing (NLP)\n",
    "\n",
    "In this lab, we will process some tweets that include \"#covid19\" to obtain their normalized versions which can be used in various analyses. The dataset we will use is obtained through filtering out non-complete tweets of [a public Kaggle dataset](https://www.kaggle.com/gpreda/covid19-tweets). You can later apply the same processes to your own datasets.\n",
    "\n",
    "## Contents\n",
    "* [Importing and cleaning data](#data)\n",
    "* [NLTK](#nltk)\n",
    "* [Tokenization](#token)\n",
    "* [Part-of-speech](#pos)\n",
    "* [Stemming and lemmatization](#stem-lemma)\n",
    "* [Negation handling](#negation)\n",
    "* [Pulling everything together](#everything)\n",
    "* [Sentiment analysis](#sentiment)\n",
    "* [Bonus: Levenshtein distance](#levenshtein)\n",
    "* [Bonus: NLP with Turkish](#tr)\n",
    "    * [Stemming](#tr-stem)\n",
    "    * [POS tagging and lemmatization](#tr-pos-lemma)\n",
    "    * [Dealing with Turkish characters](#tr-encoding)\n",
    "* [More information](#more-info)\n",
    "\n",
    "## Importing and cleaning data<a id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Praying for good health and recovery of @ChouhanShivraj . #covid19 #covidPositive                                                           \n",
       "1     July 25 #COVID19 update #TamilNadu - 6988 Discharge- 7758 People tested - 61729 Actice cases - 52273 #chennai - 1329                        \n",
       "2     Second wave of #COVID19 in Flanders..back to more #homework again... https://t.co/9swImZACMN                                                \n",
       "3     Holy water in times of #COVID19 https://t.co/YaZ49yxL27                                                                                     \n",
       "4     #FEMA acknowledges #PuertoRico lacks rebuilt homes and a hospital to survive #COVID19 https://t.co/PLXqKc5K5d                               \n",
       "5     Actionables for a healthy recovery from #COVID19 #climate https://t.co/crGlKZOz5Z                                                           \n",
       "6     Volume for those at the back please. üîä #COVID19 https://t.co/d0pn2Bv2Hx                                                                     \n",
       "7     Why has Ruto not eulogisied Mkapa!! Asking for Moses Kuria Raila #RutoSwindlingGEMA #COVID19                                                \n",
       "8     Crazy that the world has come to this but as Americans we will fight to get through this!üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏ #covid19                                \n",
       "9     @jimsciutto @JoAnnBaldwin55 People whose relatives have died from #COVID19 should file a class action lawsuit against Sinclair Broadcasting.\n",
       "10    I miss isopropyl alcohol so much!!!! Ethanol in hand sanitizer smells like I poured tequila on my hands ü§¢ #COVID19                          \n",
       "11    @SonuSood sir mom is in ICU due to COVID just want prayers from you and everyone who is listening you . #COVID19                            \n",
       "12    While you at it, please follow me https://t.co/KheirQEh6Z #Fergusons #DurbanJuly #alreadyvideo #COVID19 #WiseUp https://t.co/h1TYdOgjwv     \n",
       "13    2,000 women lawyers write to #AmitShah seeking 5 Lakh Loan per financially drained lawyer. #COVID19 #Coronavirus https://t.co/JEjVTeCjEn    \n",
       "14    let us give it a try #alreadyvideo #COVID19 #HurricaneHanna https://t.co/2HChuAfNHy                                                         \n",
       "15    The Egyptians are not got shit on this. #COVID19 https://t.co/kl5lzOJaxO                                                                    \n",
       "16    Not only is the area about to be hit by #hanna a #COVID19 hotspot, but Hurricane Harvey work is ongoing. Still.                             \n",
       "17    1.28% of the you.S. population is infected with Covid-19 #COVID19 #TrumpVirus #MaskItOrCasket                                               \n",
       "18    .@headout with a dashboard to boost post-#COVID19 #travel https://t.co/0gnRgqvLlh                                                           \n",
       "19    Highest ever number of new cases yesterday #coronavirus #covid19 #STAYatHOME https://t.co/oovQrvMviY                                        \n",
       "20    @bubbaprog 350 school employees exposed to #COVID19 in Manatee County schools over the summer https://t.co/GxiYb61ay0                       \n",
       "21    @BenJamesPhotos Italy #BenPC #LOCKDOWNPHOTOCHALLENGE #COVID19 https://t.co/27jyDBhbMc                                                       \n",
       "22    If CM of Madhya Pradesh is tested positive for COVID, then Scindia should also be tested for #COVID19.                                      \n",
       "23    Bihar witnesses biggest single-day spike of 2,803 new #Covid19 cases https://t.co/zTPksALE7T https://t.co/979SPKzt3w                        \n",
       "24    Lunch in #Amsterdam away from the mass tourists that have invaded so quickly during the #COVID19 pandemic ! #travel https://t.co/DBCPWA0Ao9 \n",
       "25    The worst type of spread. #COVID19 #COVID„Éº19 #COVIDIOTS https://t.co/zyhtCU0A4B                                                             \n",
       "26    Global fatalities chart for the last 24 hours..#COVID19 1. Bolsonaro 2. Trump 3. Modi https://t.co/D7BHp4xEE3                               \n",
       "27    How about everyone can spread #COVID19. Another study not accurate! https://t.co/8h5OC3AzYq                                                 \n",
       "28    7813 new positive #COVID19 cases reported in #AndhraPradesh from last 24hours #APFightsCorona #StayHome https://t.co/MkpUtMvtyZ             \n",
       "29    395 new cases and 3 new deaths in Uzbekistan [13:22 GMT] #coronavirus #CoronaVirusUpdate #COVID19 #CoronavirusPandemic                      \n",
       "30    Photos: #UAE hospital launches support group for #Covid19 patients https://t.co/wLnFggHSQL                                                  \n",
       "31    @RepMattGaetz @realDonaldTrump @GaetzTakes How much will dead and #COVID19 sufferers spend?                                                 \n",
       "32    Are people working in shops supposed to wear face covering? #COVID19 #England                                                               \n",
       "33    How is the US such a colossal #COVID19 failure? https://t.co/jKcTgTF67V                                                                     \n",
       "34    Rutland's own, supporting #COVID19 testing https://t.co/sVtRf11T3s                                                                          \n",
       "35    100% feeling good and 0% some symptomsüôÉ Long May it last ü§ûüèª #COVID19 ü¶†ü§õüèª https://t.co/37Dc8BweJO                                            \n",
       "36    @TheDailyEdge @seanhannity @FoxNews So he can spread lies, hate, sexual violence, and murder. #COVID19 #FoxNews                             \n",
       "37    ##COVID19 and New York retail https://t.co/aUv60GNyfE                                                                                       \n",
       "38    #COVID19: Delhi reported 1,142 new cases and 29 deaths today, taking the total number of cases to 1,29,531 and death tally to 3,806.        \n",
       "39    @UrsulaIreneRay1 @VijayShadean #SouthAfrica #tourism @GoToSouthAfrica #CarteBlanche #COVID19 #Cricket #Rugby not sure of the breed          \n",
       "40    #Brazil‚Äôs #Bilsonaro who tested positive for #COVID19 says his latest test result was negative.                                             \n",
       "41    STOP FOGGY GLASSES WITH THIS HACK!!!!!! #COVID19 #facecoverings #maskproblems #facemask #hack #maskhack #MaskMoaners https://t.co/IjuRQVuBHL\n",
       "42    #COVID19 is the religion of 21st Century Jews fighting 21st Century Christian Evangelicals                                                  \n",
       "43    @BenJamesPhotos Pink #BENPC #LOCKDOWNPHOTOCHALLENGE #COVID19 #SaturdayMorning #flowers https://t.co/DLGgosBPjQ                              \n",
       "44    My uncle just tested positive for #COVID19 , we already lost 3 family members. PLEASE wear a mask,social distance. https://t.co/YoXI0LbeLI  \n",
       "45    #APEC reaffirms #COVID19 economic recovery priorities, movement of essential goods https://t.co/sWRE9l9Fss                                  \n",
       "46    Cautious optimism? Arizona May be plateauing with #COVID19 Potential good news. https://t.co/j20GOpHK04                                     \n",
       "47    Want A #COVID19 Test? it is Much Easier To Get In Wealthier, Whiter Neighborhoods #TestingWhileBlack https://t.co/vCdDRLHFEs                \n",
       "48    The truth about #COVID19 has to be around here somewhere. #coronavirus update #SaturdayMorning #saturdayvibes https://t.co/7hiLER0fHc       \n",
       "49    Three Cs is a term I had not heard before. Good advice. #GoodAdvice #COVID19 https://t.co/ykSvJbILKP                                        \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import html\n",
    "import contractions\n",
    "\n",
    "# Importing it using Pandas:\n",
    "dataset = pd.read_csv(\"covid19_tweets_filtered.csv\")\n",
    "\n",
    "# The function we had also used while collecting some tweets, contraction\n",
    "# expansion feature added:\n",
    "def simplify_text(text):\n",
    "    # Replaces line breaks or other whitespace characters with a single space:\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Unescapes HTML so that characters like \"&\" are displayed correctly:\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Expands English contractions. Example: \"let's\" -> \"let us\"\n",
    "    # Note that it may not yield the correct result with an ambigurous \n",
    "    # contraction such as \"he's\" (it could be \"he is\" or \"he has\").\n",
    "    # By the way, it does not preserve letter cases, but we were going to \n",
    "    # lowercase them anyway.\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# It looks like tweets do not require HTML-unescaping anyway, but applying \n",
    "# simplify_text() to all texts should at least get rid of unnecessary whitespace:\n",
    "dataset[\"text\"] = dataset[\"text\"].map(simplify_text)\n",
    "\n",
    "# This option will automatically set the column width when we display data:\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "# Taking a look at the tweet contents:\n",
    "dataset[\"text\"].head(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily read these tweets, but they are far from being suitable for analysis.\n",
    "\n",
    "We can filter tweets by language using the API, and our dataset is _seemingly_ made of tweets written in English, but it can be a good idea to check their language on our end before analysis. For this, we can use langdetect package and see if there is a text that is not detected to be in English. However, these approaches may not always work, especially with noisy text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    39549\n",
       "it    151  \n",
       "fr    117  \n",
       "es    111  \n",
       "de    95   \n",
       "af    76   \n",
       "id    75   \n",
       "et    71   \n",
       "ca    59   \n",
       "nl    53   \n",
       "ro    49   \n",
       "pt    46   \n",
       "tl    42   \n",
       "sv    41   \n",
       "no    38   \n",
       "cy    23   \n",
       "so    22   \n",
       "da    21   \n",
       "sw    12   \n",
       "sl    11   \n",
       "hr    11   \n",
       "tr    9    \n",
       "fi    9    \n",
       "pl    8    \n",
       "vi    8    \n",
       "ar    5    \n",
       "lt    4    \n",
       "sq    3    \n",
       "sk    3    \n",
       "lv    3    \n",
       "hu    3    \n",
       "cs    2    \n",
       "fa    1    \n",
       "el    1    \n",
       "ko    1    \n",
       "ja    1    \n",
       "ru    1    \n",
       "      1    \n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_tweet_lang(tweet):\n",
    "    # This roughly removes user handles and URLs:\n",
    "#     tweet = ' '.join(re.sub(\"(@[A-Za-z0-9_\\-]+ )|(https?:\\/\\/.* )|(www\\..* )\",\" \",tweet).split())\n",
    "    try:\n",
    "        lang = detect(tweet)\n",
    "    except:\n",
    "        lang = \"\"\n",
    "        \n",
    "    return lang\n",
    "\n",
    "dataset[\"lang\"] = dataset[\"text\"].map(detect_tweet_lang)\n",
    "\n",
    "# Summarizes detected language counts:\n",
    "dataset[\"lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other languages are detected as well. However, it looks like they are not accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>@stayingupszn When did I ask? #COVID19 #katiehopkins #LUFC</td>\n",
       "      <td>af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>I love You !!!üòòüòòüòò #DJSBU #COVID19 https://t.co/xzKOqrAA6m</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Emptiness #cosasqueveo #COVID19 https://t.co/sgye0XMFBu</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Go away #COVID19 ! ‚Üíhttps://t.co/ou0tvwKsVF https://t.co/1qR4ealkBp</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ü¶†Covid-19 #votresant√© #COVID19 ü¶† https://t.co/zYmwFIUTvu</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>833-917-2880. ANSWER THE CALL! #Florida #COVID19 https://t.co/L8sNC6kDNj</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>Pakistan vs India Yes we won üáµüá∞ #COVID19 @MasudAKhan6 @panku_ @MajorPoonia @GeneralBakshi https://t.co/vqNTnItsUX</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Corona Baba booked in Hyderabad #CoronaBaba #Hyderabad #COVID19 https://t.co/B8bsXAMOuv</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Fun fact üêà #ketamine #covid19 #covid #coronvirus #coronavirusmemes https://t.co/rIuyKOJ0He</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>#weekendwarriors metres of @Metamarkuk media #COVID19 https://t.co/qHik4sVbfu</td>\n",
       "      <td>af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>Looking at that garden like #COVID19 https://t.co/UAK9KX5W8x</td>\n",
       "      <td>af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>#covid19 #Romania positive trend unfortunately https://t.co/FI8jFuevXA</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>@FCBfemeni @catacoll2001 Xavi tested positive for #COVID19</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>#EastGodavari today's bulletin #kakinada #rajahmundry #razole #covidupdates #COVID19 https://t.co/tciXmQ0pL7</td>\n",
       "      <td>sl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>#COVID19 priorities https://t.co/MYqQwGbQtS</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>@Officialzeez OYA LET JUST VOTE üë©‚Äçüíª WHAT SHOULD HAPPEN TO BBNAIJA?? #Nengi #BBNaijia2020 #COVID19 #lucy #doro</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>Get well soon mamu #Covid19 https://t.co/b2AXrWgwWx</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>DUSK Weekly Goals! #kuwait #kuwaitmoms #COVID19 #kuwaitschool #schoolsinkuwait #americanschool https://t.co/fdmgjx0vTm</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>Please please take care, hmmm dah naik balik dah #covid19</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Xavi tests positive for COVID-19 ü•∫ #Xavi #COVID19</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       text  \\\n",
       "89   @stayingupszn When did I ask? #COVID19 #katiehopkins #LUFC                                                               \n",
       "206  I love You !!!üòòüòòüòò #DJSBU #COVID19 https://t.co/xzKOqrAA6m                                                                \n",
       "348  Emptiness #cosasqueveo #COVID19 https://t.co/sgye0XMFBu                                                                  \n",
       "478  Go away #COVID19 ! ‚Üíhttps://t.co/ou0tvwKsVF https://t.co/1qR4ealkBp                                                      \n",
       "500  ü¶†Covid-19 #votresant√© #COVID19 ü¶† https://t.co/zYmwFIUTvu                                                                 \n",
       "554  833-917-2880. ANSWER THE CALL! #Florida #COVID19 https://t.co/L8sNC6kDNj                                                 \n",
       "560  Pakistan vs India Yes we won üáµüá∞ #COVID19 @MasudAKhan6 @panku_ @MajorPoonia @GeneralBakshi https://t.co/vqNTnItsUX        \n",
       "566  Corona Baba booked in Hyderabad #CoronaBaba #Hyderabad #COVID19 https://t.co/B8bsXAMOuv                                  \n",
       "577  Fun fact üêà #ketamine #covid19 #covid #coronvirus #coronavirusmemes https://t.co/rIuyKOJ0He                               \n",
       "579  #weekendwarriors metres of @Metamarkuk media #COVID19 https://t.co/qHik4sVbfu                                            \n",
       "653  Looking at that garden like #COVID19 https://t.co/UAK9KX5W8x                                                             \n",
       "683  #covid19 #Romania positive trend unfortunately https://t.co/FI8jFuevXA                                                   \n",
       "717  @FCBfemeni @catacoll2001 Xavi tested positive for #COVID19                                                               \n",
       "868  #EastGodavari today's bulletin #kakinada #rajahmundry #razole #covidupdates #COVID19 https://t.co/tciXmQ0pL7             \n",
       "872  #COVID19 priorities https://t.co/MYqQwGbQtS                                                                              \n",
       "887  @Officialzeez OYA LET JUST VOTE üë©‚Äçüíª WHAT SHOULD HAPPEN TO BBNAIJA?? #Nengi #BBNaijia2020 #COVID19 #lucy #doro            \n",
       "897  Get well soon mamu #Covid19 https://t.co/b2AXrWgwWx                                                                      \n",
       "946  DUSK Weekly Goals! #kuwait #kuwaitmoms #COVID19 #kuwaitschool #schoolsinkuwait #americanschool https://t.co/fdmgjx0vTm   \n",
       "988  Please please take care, hmmm dah naik balik dah #covid19                                                                \n",
       "990  Xavi tests positive for COVID-19 ü•∫ #Xavi #COVID19                                                                        \n",
       "\n",
       "    lang  \n",
       "89   af   \n",
       "206  fr   \n",
       "348  pt   \n",
       "478  so   \n",
       "500  fr   \n",
       "554  pt   \n",
       "560  id   \n",
       "566  so   \n",
       "577  fr   \n",
       "579  af   \n",
       "653  af   \n",
       "683  it   \n",
       "717  it   \n",
       "868  sl   \n",
       "872  es   \n",
       "887  pl   \n",
       "897  nl   \n",
       "946  nl   \n",
       "988  id   \n",
       "990  ca   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieves the text and detected language of the first 20 tweets that are \n",
    "# seemingly not in English:\n",
    "dataset.loc[dataset[\"lang\"] != \"en\",[\"text\", \"lang\"]].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very small percentage is indeed in a different language, but the rest are being misclassified even when we remove URLs and user handles before checking, or look at the top three possible languages using `detect_langs()` from the same package. Fortunately, the percentage is not that significant. If you want, you can remove these tweets from your dataset just to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you insist on removing tweets that \"appear\" to be written in a \n",
    "# different language.\n",
    "# dataset = dataset[dataset[\"lang\"] == \"en\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK<a id=\"nltk\"></a>\n",
    "\n",
    "For NLP with English texts, we will mostly use Natural Language Toolkit (NLTK) package. It is one of the most popular NLP packages for Python, and it has many useful tools. If you have Anaconda, it should come with it. If you do not have it, check [this](https://riptutorial.com/nltk#installation-or-setup) page for installation. We also need to manually download some datasets that it will use. If you have not already downloaded them, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "download(\"popular\") # Popular datasets\n",
    "download('tagsets') # Tagsets for POS tagging\n",
    "download('vader_lexicon') # Lexicon for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization<a id=\"token\"></a>\n",
    "\n",
    "To be able to analyze the data, we need to split each text into small units. This process is called \"tokenization.\" It is mostly done by splitting the text using whitespace and punctuation, but there are different tokenizers that handle certain details slightly differently. NLTK has a tweet-specific tokenizer that preserves \"#\" and \"@\" characters while other tokenizers can mess up Twitter-specific notations. See how tweets are tokenized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Praying for good health and recovery of @ChouhanShivraj . #covid19 #covidPositive \n",
      "‚îî‚îÄ> ['praying', 'for', 'good', 'health', 'and', 'recovery', 'of', '@chouhanshivraj', '.', '#covid19', '#covidpositive'] \n",
      "\n",
      "July 25 #COVID19 update #TamilNadu - 6988 Discharge- 7758 People tested - 61729 Actice cases - 52273 #chennai - 1329 \n",
      "‚îî‚îÄ> ['july', '25', '#covid19', 'update', '#tamilnadu', '-', '6988', 'discharge', '-', '7758', 'people', 'tested', '-', '61729', 'actice', 'cases', '-', '52273', '#chennai', '-', '1329'] \n",
      "\n",
      "Second wave of #COVID19 in Flanders..back to more #homework again... https://t.co/9swImZACMN \n",
      "‚îî‚îÄ> ['second', 'wave', 'of', '#covid19', 'in', 'flanders', '..', 'back', 'to', 'more', '#homework', 'again', '...', 'https://t.co/9swImZACMN'] \n",
      "\n",
      "Holy water in times of #COVID19 https://t.co/YaZ49yxL27 \n",
      "‚îî‚îÄ> ['holy', 'water', 'in', 'times', 'of', '#covid19', 'https://t.co/YaZ49yxL27'] \n",
      "\n",
      "#FEMA acknowledges #PuertoRico lacks rebuilt homes and a hospital to survive #COVID19 https://t.co/PLXqKc5K5d \n",
      "‚îî‚îÄ> ['#fema', 'acknowledges', '#puertorico', 'lacks', 'rebuilt', 'homes', 'and', 'a', 'hospital', 'to', 'survive', '#covid19', 'https://t.co/PLXqKc5K5d'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# This tokenizer will tokenize tweets. \"preserve_case\" parameter can be used to \n",
    "# preserve cases or make it all lowercase. \"reduce_len\" parameter shortens\n",
    "# consecutive character repetitions to at most three consecutive repetitions to\n",
    "# reduce noise.\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "\n",
    "for tweet in dataset[\"text\"].head():\n",
    "    print(tweet,\"\\n‚îî‚îÄ>\",tokenizer.tokenize(tweet),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech<a id=\"pos\"></a>\n",
    "\n",
    "Before we move along, we should take a look at part-of-speech (commonly referred as \"POS\") tagging. Sometimes we need to classify words according to their function (part-of-speech) in the sentence, so that we can extract certain information. For example, if we need to analyze verbs in a long text, we can use words' POS tags and filter out words that are not verbs, which would significantly simplify the process. POS tags are especially useful when a word can have different functions in a sentence with the exact same form, so we cannot just take a look at the word itself and draw conclusions. For example, \"type\" can mean a category or a verb (to type). For these reasons, we have POS taggers. A POS tagger classifies each unit's syntactic function in the sentence. There are different types of POS taggers. The one we will use is actually a pre-trained machine learning classifier of NLTK. The perceptron model is trained with a [treebank](https://en.wikipedia.org/wiki/Treebank) (a corpus with annotated POS tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('which', 'WDT'), ('type', 'NN'), ('of', 'IN'), ('typewriter', 'NN'), ('would', 'MD'), ('you', 'PRP'), ('like', 'VB'), ('to', 'TO'), ('type', 'VB'), ('with', 'IN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sentence = \"Which type of typewriter would you like to type with?\"\n",
    "\n",
    "# Uppercase letters can confuse POS tagging, so we need to lowercase everything. This \n",
    "# is automatically handled by our tokenizer anyway. Note that a truecasing approach or \n",
    "# a more simplified approach such as only touching the first letter of a sentence could \n",
    "# potentially yield better results in POS tagging. You can check \n",
    "# https://en.wikipedia.org/wiki/Truecasing and \n",
    "# https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21\n",
    "# to read more about truecasing.\n",
    "sentence_tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# POS tagging:\n",
    "sentence_tokens_pos = pos_tag(sentence_tokens)\n",
    "\n",
    "print(sentence_tokens_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each term is now classified. However, the tags are not very clear for us. We can check the documentation or use a dictionary to read the explanation and see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: which \n",
      "POS tag: WDT \n",
      "Explanation: WH-determiner \n",
      "Example: that what whatever which whichever  \n",
      "\n",
      "Token: type \n",
      "POS tag: NN \n",
      "Explanation: noun, common, singular or mass \n",
      "Example: common-carrier cabbage knuckle-duster Casino afghan shed thermostat investment slide humour falloff slick wind hyena override subhumanity machinist ...  \n",
      "\n",
      "Token: of \n",
      "POS tag: IN \n",
      "Explanation: preposition or conjunction, subordinating \n",
      "Example: astride among uppon whether out inside pro despite on by throughout below within for towards near behind atop around if like until below next into if beside ...  \n",
      "\n",
      "Token: typewriter \n",
      "POS tag: NN \n",
      "Explanation: noun, common, singular or mass \n",
      "Example: common-carrier cabbage knuckle-duster Casino afghan shed thermostat investment slide humour falloff slick wind hyena override subhumanity machinist ...  \n",
      "\n",
      "Token: would \n",
      "POS tag: MD \n",
      "Explanation: modal auxiliary \n",
      "Example: can cannot could couldn't dare may might must need ought shall should shouldn't will would  \n",
      "\n",
      "Token: you \n",
      "POS tag: PRP \n",
      "Explanation: pronoun, personal \n",
      "Example: hers herself him himself hisself it itself me myself one oneself ours ourselves ownself self she thee theirs them themselves they thou thy us  \n",
      "\n",
      "Token: like \n",
      "POS tag: VB \n",
      "Explanation: verb, base form \n",
      "Example: ask assemble assess assign assume atone attention avoid bake balkanize bank begin behold believe bend benefit bevel beware bless boil bomb boost brace break bring broil brush build ...  \n",
      "\n",
      "Token: to \n",
      "POS tag: TO \n",
      "Explanation: \"to\" as preposition or infinitive marker \n",
      "Example: to  \n",
      "\n",
      "Token: type \n",
      "POS tag: VB \n",
      "Explanation: verb, base form \n",
      "Example: ask assemble assess assign assume atone attention avoid bake balkanize bank begin behold believe bend benefit bevel beware bless boil bomb boost brace break bring broil brush build ...  \n",
      "\n",
      "Token: with \n",
      "POS tag: IN \n",
      "Explanation: preposition or conjunction, subordinating \n",
      "Example: astride among uppon whether out inside pro despite on by throughout below within for towards near behind atop around if like until below next into if beside ...  \n",
      "\n",
      "Token: ? \n",
      "POS tag: . \n",
      "Explanation: sentence terminator \n",
      "Example: . ! ?  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.data import load\n",
    "tag_dict = load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "for token in sentence_tokens_pos:\n",
    "    print(\"Token:\",token[0],\"\\nPOS tag:\",token[1],\"\\nExplanation:\",tag_dict[token[1]][0],\"\\nExample:\",tag_dict[token[1]][1],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the first \"type\" is classified as a noun (\"NN\") while the last one is classified as a verb (\"VB\"). It is not bad for general purposes. This will come handy later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization<a id=\"stem-lemma\"></a>\n",
    "\n",
    "Consider these two sentences:\n",
    "* Vegetables are healthy.\n",
    "* This vegetable is healthier.\n",
    "\n",
    "They both are about vegetables being healthy, but the words do not exactly occur in the same form (\"vegetables\" vs. \"vegetable\" and \"healthy\" vs. \"healthier\"). We need to simplify and conflate them to improve our analyses. One easy way to simplify words is using a stemmer. Stemmers rely on certain language-specific linguistic rules and sets of suffixes to get rid of suffixes (such as \"tional\" or \"ism\") and find the stem (root) of each word. Just like tokenizers, there are different stemmers. We will use Snowball stemmer included in NLTK here. To learn more about the original Snowball stemmer algorithm and its implementation, you can look at [this web page](http://snowball.tartarus.org/algorithms/porter/stemmer.html). See how these sentences are stemmed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['veget', 'are', 'healthi', '.']\n",
      "['this', 'veget', 'is', 'healthier', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "sentences = [\"Vegetables are healthy.\", \"This vegetable is healthier.\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_tokens = tokenizer.tokenize(sentence)\n",
    "    sentence_stems = [stemmer.stem(token) for token in sentence_tokens]\n",
    "    print(sentence_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did not exactly work for \"healthier\" the way we expected. Vegetables are now in the same form (\"veget\"), but we may not prefer to stem these words all the way to their etymologic roots. Look what happens when we use stemming with this sentence:\n",
    "* Vegetation is an assemblage of plant species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['veget', 'is', 'an', 'assemblag', 'of', 'plant', 'speci', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(token) for token in tokenizer.tokenize(\"Vegetation is an assemblage of plant species.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Vegetation\" becomes \"veget\" as well. This is not wrong, but probably not what we were going for either. Instead, we can use lemmatization. Lemmatization is the process of obtaining a word's base form. It is more complex than stemming, and it can also use a term's POS tag to determine the correct form. If you do not provide a POS tag, it assumes the term is a noun. One problem is that we cannot directly feed the tag we obtain through `pos_tag()` to the lemmatizer, so we need to write a function to simplify the tag and pas it in the form it requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vegetable', 'be', 'healthy', '.']\n",
      "['this', 'vegetable', 'be', 'healthy', '.']\n",
      "['vegetation', 'be', 'an', 'assemblage', 'of', 'plant', 'specie', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_lemmatizer_pos(pos):\n",
    "    pos_start = pos[0] # Takes the first letter to simplify the POS tag\n",
    "    if pos_start == \"J\":\n",
    "        return wn.ADJ\n",
    "    elif pos_start == \"V\":\n",
    "        return wn.VERB\n",
    "    elif pos_start == \"R\":\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN \n",
    "\n",
    "sentences = [\"Vegetables are healthy.\", \"This vegetable is healthier.\", \"Vegetation is an assemblage of plant species.\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Lemmatization does not automatically converts it to lowercase, so we are manually \n",
    "    # doing it here:\n",
    "    sentence_tokens = tokenizer.tokenize(sentence)\n",
    "    sentence_tokens_pos = pos_tag(sentence_tokens)\n",
    "#     print(sentence_tokens_pos)\n",
    "    # Notice that we pass a token's POS tag to the lemmatizer through get_lemmatizer_pos()\n",
    "    sentence_lemmas = [lemmatizer.lemmatize(token[0], pos=get_lemmatizer_pos(token[1])) for token in sentence_tokens_pos]\n",
    "    # Uncomment the line below to see how it works when we do not provide a POS tag (it \n",
    "    # lowers the success):\n",
    "#     sentence_lemmas = [lemmatizer.lemmatize(token[0]) for token in sentence_tokens_pos]\n",
    "    print(sentence_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"vegetables\" and \"vegetable\" become \"vegetable\" while \"vegetation\" is not conflated with them.\n",
    "* \"healthy\" and \"healthier\" become \"healthy.\"\n",
    "* \"is\" and \"are\" become \"be.\"\n",
    "\n",
    "Note that lemmatization may not always work correctly either.\n",
    "\n",
    "## Negation handling<a id=\"negation\"></a>\n",
    "\n",
    "What if we have a negation? Consider the following sentences:\n",
    "\n",
    "* Butter is healthy.\n",
    "* Butter is not healthy.\n",
    "\n",
    "While it is clear for us that one is the opposite of the other, this can get tricky after tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['butter', 'be', 'healthy', '.']\n",
      "['butter', 'be', 'not', 'healthy', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Butter is healthy.\", \"Butter is not healthy.\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_tokens_pos = pos_tag(tokenizer.tokenize(sentence))\n",
    "    sentence_lemmas = [lemmatizer.lemmatize(token[0], pos=get_lemmatizer_pos(token[1])) for token in sentence_tokens_pos]\n",
    "    print(sentence_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have many comments on butter and we want to analyze the answers to decide whether butter is healthy. If we simply look at the word occurrences, the word \"healthy\" occurs twice here, but one of the sentences actually states that butter is not healthy. Negations could mess up some analyses and go unnoticed. To quickly deal with it, we can handle negations using `mark_negation()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['butter', 'be', 'healthy', '.']\n",
      "['butter', 'be', 'not', 'healthy_NEG', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_tokens_pos = pos_tag(tokenizer.tokenize(sentence))\n",
    "    sentence_lemmas = [lemmatizer.lemmatize(token[0], pos=get_lemmatizer_pos(token[1])) for token in sentence_tokens_pos]\n",
    "    # Negations are marked here:\n",
    "    sentence_lemmas_negated = mark_negation(sentence_lemmas)\n",
    "    print(sentence_lemmas_negated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the second \"healthy\" is now \"healthy_NEG.\"\n",
    "\n",
    "What if we have a more complex sentences? The negation only applies to \"healthy\" in these sentences:\n",
    "* Butter is not healthy but delicious.\n",
    "* Butter is not healthy, but it is delicious.\n",
    "* Butter is not healthy. However, it is delicious.\n",
    "\n",
    "However, it is not that smart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['butter', 'be', 'not', 'healthy_NEG', 'but_NEG', 'delicious_NEG', '.']\n",
      "['butter', 'be', 'not', 'healthy_NEG', ',_NEG', 'but_NEG', 'it_NEG', 'be_NEG', 'delicious_NEG', '.']\n",
      "['butter', 'be', 'not', 'healthy_NEG', '.', 'however', ',', 'it', 'be', 'delicious', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Butter is not healthy but delicious.\", \"Butter is not healthy, but it is delicious.\", \"Butter is not healthy. However, it is delicious.\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_tokens_pos = pos_tag(tokenizer.tokenize(sentence))\n",
    "    sentence_lemmas = mark_negation([lemmatizer.lemmatize(token[0], pos=get_lemmatizer_pos(token[1])) for token in sentence_tokens_pos])\n",
    "    print(sentence_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All consequent tokens up until the sentence terminator (such as period) are marked. Although the sentences state that butter is delicious, it is marked as negated in the first and the second sentences.\n",
    "\n",
    "It can also handle words like \"never,\" but it cannot handle implicit or subtle negations. These problems require more sophisticated approaches. Sarcasm detection is a research field by itself in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['butter', 'be', 'never', 'healthy_NEG', '.']\n",
      "['butter', 'be', 'rarely', 'healthy', '.']\n",
      "['yeah', ',', 'sure', ',', 'butter', 'be', 'healthy', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Butter is never healthy.\", # Negated\n",
    "             \"Butter is rarely healthy.\", # Mostly unhealthy\n",
    "             \"Yeah, sure, butter is healthy.\" # Sarcastic\n",
    "            ]\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_tokens_pos = pos_tag(tokenizer.tokenize(sentence))\n",
    "    sentence_lemmas = mark_negation([lemmatizer.lemmatize(token[0], pos=get_lemmatizer_pos(token[1])) for token in sentence_tokens_pos])\n",
    "    print(sentence_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have probably realized that these basic approaches are not fool-proof. We rely on these methods to work correctly _most of the time_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling everything together<a id=\"everything\"></a>\n",
    "\n",
    "We can now combine these methods to normalize the tweets in our dataset. However, note that tweets have many other elements to consider as well:\n",
    "\n",
    "* Hashtags\n",
    "* User handles (usernames)\n",
    "* URLs\n",
    "* Numbers\n",
    "* Stop words (like \"the\" and \"is\")\n",
    "* Emojis\n",
    "* Punctuations\n",
    "* Typographic errors\n",
    "\n",
    "In certain cases, we cannot learn much from them, so removing them would reduce the noise. In some cases, they can provide valuable information. For example, emojis can convey a message, so one might prefer to keep them. While some of the elements would wildly vary and make too much noise, indicators for their existence could be helpful as well. So, we could conflate them under a general type-specific token such as \"\\<number\\>.\" Keeping specific user handles could come handy for social network analysis, but our focus here is text analysis. The existence of a question or exclamation mark can be informative as well, but we will not specifically analyze them. These removals and such are done after lemmatization to preserve the sentence structure that is helpful for POS tagging.\n",
    "\n",
    "We can automatically correct typographic errors to reduce the noise even further. An easy way one can do it is by using a dictionary and finding the closest word for words that do not exist in the dictionary. We can use \"autocorrect\" package for this. Note that since the dictionary may not be that extensive while tweets can be quite colloquial and noisy, it would probably be actually harmful. Therefore, we will not use it, but you can pass `autocorrect=True` below and see how it affects your results. This process makes use of [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance), which we will talk about later.\n",
    "\n",
    "We can write a function to streamline these processes. The function below takes a tweet and applies all of the processes in order. It also has some parameters that affect how certain tokens are handled. By default:\n",
    "\n",
    "* Numbers are directly removed instead of being conflated under \"\\<number\\>\"\n",
    "* Users are directly removed instead of being conflated under \"\\<user\\>\"\n",
    "* Hashtags are directly removed instead of being conflated under \"\\<hashtag\\>\"\n",
    "* URLs are directly removed instead of being conflated under \"\\<url\\>\"\n",
    "* Emojis are removed\n",
    "* Punctuations are removed\n",
    "* Negations are marked\n",
    "* Autocorrection is off\n",
    "\n",
    "We can change these behaviors on the go by specifying some parameters.\n",
    "\n",
    "While processing the tweets, we might as well extract n-grams. N-grams are n-many units that consecutively occur in a sentence. They can be more useful than simply looking at individual units. For example, individually looking at the words \"positive\" and \"test\" lose their collective meaning compared to \"positive test.\" It is rather obvious when we look at the unigrams (individual units) of a single sentence, but it may not be as clear when we aggregate all of the data. N-grams help us retain more information. When these consecutive unit groups are duples, they are called bigrams (2-grams). Consider the following text: \n",
    "\n",
    "`This is a sentence. This one too.`\n",
    "\n",
    "The bigrams would be as follows:\n",
    "\n",
    "* <span style=\"background-color:#9e0059; color:white\">This</span> - <span style=\"background-color:#118ab2; color:white\">is</span>\n",
    "* <span style=\"background-color:#118ab2; color:white\">is</span> - <span style=\"background-color:#553739; color:white\">a</span>\n",
    "* <span style=\"background-color:#553739; color:white\">a</span> - <span style=\"background-color:#8338ec; color:white\">sentence</span>\n",
    "* <span style=\"background-color:#cf995f; color:white\">This</span> - <span style=\"background-color:#43aa8b; color:white\">one</span>\n",
    "* <span style=\"background-color:#43aa8b; color:white\">one</span> - <span style=\"background-color:#577590; color:white\">too</span>\n",
    "\n",
    "Punctuations are mostly not included in n-grams. Also note that \"sentence\" and the second \"this\" do not constitute a bigram as we look for consecutive units within the same sentence. \n",
    "\n",
    "We can actually use NLTK to extract bigrams in this manner, but it may not be as straightforward as shown above if we are not interested in all the units. For example, we would prefer to not include the stop words since they would not give much information anyway, and our bigrams would be bloated. We also want them to be normalized to decrease the noise, so we need to process the units first. At the same time, we do not want to pair two units that are not consecutive in the original sentence. Therefore, our function needs to extract the bigrams as it processes the tokens. One alternative would be storing unit index with the filtered lemmas which can be later used for n-gram extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "import re\n",
    "import nltk.data\n",
    "from nltk import pos_tag_sents\n",
    "# If you have Anaconda, you can install emoji using \n",
    "# \"conda install -c conda-forge emoji\" command. You can download autocorrect using pip\n",
    "# and \"target\" parameter: \"pip install autocorrect --target=<directory>\"\n",
    "from autocorrect import Speller\n",
    "from emoji import get_emoji_regexp\n",
    "\n",
    "# Note that it looks like the POS tagger prefers us to feed sentences separately (or \n",
    "# feed them as a list to pos_tag_sents()). Therefore, we will tokenize sentences first.\n",
    "# Sentence tokenizer tokenizes sentences while also trying to handle periods that do not\n",
    "# function as a sentence terminator (such as the period in \"Mr.\").\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# This function wraps up all the processes and returns normalized tweets (optionally with\n",
    "# bigrams). It has some parameters you can play with. To keep it monolithic and easier to\n",
    "# analyze, it is written as a one big function. From a software engineering perspective, \n",
    "# it would make more sense to move certain parts to their own functions to separate \n",
    "# different concerns (subtasks) such as emoji removal, punctutation removal, etc. The \n",
    "# function also has tokenizer parameters that have default values, which make sure that \n",
    "# these objects exist when it needs them. You can also include import statements and such \n",
    "# in the function to make it more portable, or you could make it a module.\n",
    "def tokenize_normalize(tweet, sentence_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle'),\n",
    "                       tokenizer=TweetTokenizer(preserve_case=False), return_bigrams=False, \n",
    "                       tokenize_numbers=False, tokenize_users=False, tokenize_hashtags=False,\n",
    "                       tokenize_urls=False, remove_emoji=True, remove_punct=True, \n",
    "                       handle_negation=True, autocorrect=False):\n",
    "    \n",
    "    # This retrieves a list of stop words in English, which will be used to remove the \n",
    "    # stop words:\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    # These combined punctuations will be used to remove punctuations from tweets (it \n",
    "    # is an extension to string.punctuation):\n",
    "    punctuations = \"!\\\"‚Äú‚Äù#$%&'‚Äò‚Äô()*+,-./:;<=>?@[\\]^_`{|}~‚Äç\"\n",
    "    \n",
    "    # We will use this function to correct typographic errors:\n",
    "    if autocorrect and \"autocorrect\" in sys.modules:\n",
    "        spell = Speller()\n",
    "\n",
    "    # Separates tweets into sentences:\n",
    "    tweet_sentences = sentence_tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Tokenization outputs are kept in separate lists for each sentence:\n",
    "    tweet_sentences_tokens = [tokenizer.tokenize(sentence) for sentence in tweet_sentences]\n",
    "    \n",
    "    # POS tagging happens separately for each sentence before they are combined:\n",
    "    tokens_pos = [pos_tag for pos_tags in pos_tag_sents(tweet_sentences_tokens) for pos_tag in pos_tags]\n",
    "    \n",
    "    # For each POS-tagged token, a lemma is obtained:\n",
    "    lemmas = [lemmatizer.lemmatize(token[0], pos=get_lemmatizer_pos(token[1])) for token in tokens_pos]\n",
    "#     print(lemmas)\n",
    "    \n",
    "    # Marks negations:\n",
    "    if handle_negation:\n",
    "        lemmas = mark_negation(lemmas)\n",
    "    \n",
    "    filtered_lemmas = []\n",
    "    bigrams = []\n",
    "    last_filtered_lemma_index = None\n",
    "    last_filtered_lemma = None\n",
    "    for lemma_index, lemma in enumerate(lemmas):\n",
    "        \n",
    "        # The amount of emojis has skyrocketed, and the way new emojis or their \n",
    "        # varients are added technically complicates handling emojis. For example, \n",
    "        # some emojis are formed by combining different emojis and a zero-width joiner \n",
    "        # in between. Removing variation selectors such as hair/skin color and gender \n",
    "        # for emojis since they cause noise and tokenization problems:\n",
    "        if re.sub(\"[\\\\uFE00-\\\\uFE0F‚ôÇ‚ôÄ‚Äç]+\", \"\", lemma) == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Filters hashtags:\n",
    "        if lemma.startswith(\"#\"):\n",
    "            if tokenize_hashtags:\n",
    "                lemma = \"<hashtag>\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        # Filters user handles:\n",
    "        if lemma.startswith(\"@\"):\n",
    "            if tokenize_users:\n",
    "                lemma = \"<user>\"\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Filters stop words (considers negations):\n",
    "        if lemma.replace(\"_NEG\", \"\") in stop_words:\n",
    "            continue\n",
    "            \n",
    "        # Filters the lemma by searching for \"https://,\" \"http://,\" or \"www.\" using \n",
    "        # regular expression. If one of them exists, they are not retrieved. Regular \n",
    "        # expression may seem daunting at first. It is not mandatory, but you can check \n",
    "        # tutorials like this: https://regexone.com/lesson/introduction_abcs\n",
    "        if re.search(\"(https?:\\/\\/)|(www\\.)\", lemma):\n",
    "            if tokenize_urls:\n",
    "                lemma = \"<url>\"\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "        # Filters emojis using emeji package (considers negations):\n",
    "        if remove_emoji and \"emoji\" in sys.modules:\n",
    "            lemma = get_emoji_regexp().sub(u'', lemma.replace(\"_NEG\", \"\"))\n",
    "            \n",
    "        # Filters punctuation (considers negations):\n",
    "        if remove_punct and lemma.replace(\"_NEG\", \"\").translate(lemma.maketrans('', '', punctuations)) == \"\":\n",
    "            continue\n",
    "            \n",
    "        # Corrects typographic errors using autocorrect package (considers negations):\n",
    "        if autocorrect and \"autocorrect\" in sys.modules and spell:\n",
    "            if \"_NEG\" in lemma:\n",
    "                # Removing \"_NEG\" and adding it back after autocorrection:\n",
    "                lemma_autocorrected = spell(lemma.replace(\"_NEG\", \"\")).join(\"_NEG\")\n",
    "            else:\n",
    "                lemma_autocorrected = spell(lemma)\n",
    "                \n",
    "            if lemma != lemma_autocorrected:\n",
    "#                 print(lemma,\"autocorrected to\",lemma_autocorrected) # Uncomment this line to print the corrections\n",
    "                lemma = lemma_autocorrected\n",
    "    \n",
    "        # Tries to convert a number from string to float while also handling commas \n",
    "        # and percentage signs. If the token is a number, it is transformed to \"<number>\" \n",
    "        # token or not retrieved. If not, it silently ignores the exception and \n",
    "        # continues.\n",
    "        try:\n",
    "            float(lemma.replace(\",\", \"\").replace(\"%\", \"\"))\n",
    "            if tokenize_numbers:\n",
    "                lemma = \"<number>\"\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # If the lemma survives all these processes, it is appended to the list\n",
    "        filtered_lemmas.append(lemma)\n",
    "        \n",
    "        # If returning bigrams is set to True, this part extracts the bigrams:\n",
    "        if return_bigrams:\n",
    "            # If there is a last filtered lemma, if its location in the sentences is \n",
    "            # right before the current lemma, and if the current lemma is not a \n",
    "            # punctuation:\n",
    "            if last_filtered_lemma and last_filtered_lemma_index + 1 == lemma_index and\\\n",
    "            lemma.replace(\"_NEG\", \"\").translate(lemma.maketrans('', '', punctuations)) != \"\":\n",
    "                # The lemma group (bigram) is appended to the bigram list\n",
    "                bigrams.append([last_filtered_lemma, lemma])\n",
    "                \n",
    "            last_filtered_lemma_index = lemma_index\n",
    "            last_filtered_lemma = lemma\n",
    "    \n",
    "    if return_bigrams:\n",
    "        # It returns filtered lemmas and bigrams together\n",
    "        return (filtered_lemmas, bigrams)\n",
    "    else:\n",
    "        return filtered_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use this function to process the whole dataset.\n",
    "\n",
    "It may take a while to finish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "      <th>text_bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Praying for good health and recovery of @ChouhanShivraj . #covid19 #covidPositive</td>\n",
       "      <td>[pray, good, health, recovery]</td>\n",
       "      <td>[[good, health]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>July 25 #COVID19 update #TamilNadu - 6988 Discharge- 7758 People tested - 61729 Actice cases - 52273 #chennai - 1329</td>\n",
       "      <td>[july, update, discharge, people, test, actice, case]</td>\n",
       "      <td>[[people, test], [actice, case]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Second wave of #COVID19 in Flanders..back to more #homework again... https://t.co/9swImZACMN</td>\n",
       "      <td>[second, wave, flanders, back]</td>\n",
       "      <td>[[second, wave]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Holy water in times of #COVID19 https://t.co/YaZ49yxL27</td>\n",
       "      <td>[holy, water, time]</td>\n",
       "      <td>[[holy, water]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#FEMA acknowledges #PuertoRico lacks rebuilt homes and a hospital to survive #COVID19 https://t.co/PLXqKc5K5d</td>\n",
       "      <td>[acknowledge, lack, rebuild, home, hospital, survive]</td>\n",
       "      <td>[[lack, rebuild], [rebuild, home]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Actionables for a healthy recovery from #COVID19 #climate https://t.co/crGlKZOz5Z</td>\n",
       "      <td>[actionables, healthy, recovery]</td>\n",
       "      <td>[[healthy, recovery]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Volume for those at the back please. üîä #COVID19 https://t.co/d0pn2Bv2Hx</td>\n",
       "      <td>[volume, back, please]</td>\n",
       "      <td>[[back, please]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Why has Ruto not eulogisied Mkapa!! Asking for Moses Kuria Raila #RutoSwindlingGEMA #COVID19</td>\n",
       "      <td>[ruto, eulogisied, mkapa, ask, moses, kuria, raila]</td>\n",
       "      <td>[[eulogisied, mkapa], [moses, kuria], [kuria, raila]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Crazy that the world has come to this but as Americans we will fight to get through this!üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏ #covid19</td>\n",
       "      <td>[crazy, world, come, american, fight, get]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@jimsciutto @JoAnnBaldwin55 People whose relatives have died from #COVID19 should file a class action lawsuit against Sinclair Broadcasting.</td>\n",
       "      <td>[people, whose, relative, die, file, class, action, lawsuit, sinclair, broadcasting]</td>\n",
       "      <td>[[people, whose], [whose, relative], [class, action], [action, lawsuit], [sinclair, broadcasting]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I miss isopropyl alcohol so much!!!! Ethanol in hand sanitizer smells like I poured tequila on my hands ü§¢ #COVID19</td>\n",
       "      <td>[miss, isopropyl, alcohol, much, ethanol, hand, sanitizer, smell, like, pour, tequila, hand]</td>\n",
       "      <td>[[miss, isopropyl], [isopropyl, alcohol], [hand, sanitizer], [sanitizer, smell], [smell, like], [pour, tequila]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@SonuSood sir mom is in ICU due to COVID just want prayers from you and everyone who is listening you . #COVID19</td>\n",
       "      <td>[sir, mom, icu, due, covid, want, prayer, everyone, listen]</td>\n",
       "      <td>[[sir, mom], [icu, due], [want, prayer]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>While you at it, please follow me https://t.co/KheirQEh6Z #Fergusons #DurbanJuly #alreadyvideo #COVID19 #WiseUp https://t.co/h1TYdOgjwv</td>\n",
       "      <td>[please, follow]</td>\n",
       "      <td>[[please, follow]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2,000 women lawyers write to #AmitShah seeking 5 Lakh Loan per financially drained lawyer. #COVID19 #Coronavirus https://t.co/JEjVTeCjEn</td>\n",
       "      <td>[woman, lawyer, write, seek, lakh, loan, per, financially, drain, lawyer]</td>\n",
       "      <td>[[woman, lawyer], [lawyer, write], [lakh, loan], [loan, per], [per, financially], [financially, drain], [drain, lawyer]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>let us give it a try #alreadyvideo #COVID19 #HurricaneHanna https://t.co/2HChuAfNHy</td>\n",
       "      <td>[let, u, give, try]</td>\n",
       "      <td>[[let, u], [u, give]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Egyptians are not got shit on this. #COVID19 https://t.co/kl5lzOJaxO</td>\n",
       "      <td>[egyptian, get, shit]</td>\n",
       "      <td>[[get, shit]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Not only is the area about to be hit by #hanna a #COVID19 hotspot, but Hurricane Harvey work is ongoing. Still.</td>\n",
       "      <td>[area, hit, hotspot, hurricane, harvey, work, ongoing, still]</td>\n",
       "      <td>[[hurricane, harvey], [harvey, work]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.28% of the you.S. population is infected with Covid-19 #COVID19 #TrumpVirus #MaskItOrCasket</td>\n",
       "      <td>[population, infect, covid]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.@headout with a dashboard to boost post-#COVID19 #travel https://t.co/0gnRgqvLlh</td>\n",
       "      <td>[dashboard, boost, post]</td>\n",
       "      <td>[[boost, post]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Highest ever number of new cases yesterday #coronavirus #covid19 #STAYatHOME https://t.co/oovQrvMviY</td>\n",
       "      <td>[high, ever, number, new, case, yesterday]</td>\n",
       "      <td>[[high, ever], [ever, number], [new, case], [case, yesterday]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text  \\\n",
       "0   Praying for good health and recovery of @ChouhanShivraj . #covid19 #covidPositive                                                              \n",
       "1   July 25 #COVID19 update #TamilNadu - 6988 Discharge- 7758 People tested - 61729 Actice cases - 52273 #chennai - 1329                           \n",
       "2   Second wave of #COVID19 in Flanders..back to more #homework again... https://t.co/9swImZACMN                                                   \n",
       "3   Holy water in times of #COVID19 https://t.co/YaZ49yxL27                                                                                        \n",
       "4   #FEMA acknowledges #PuertoRico lacks rebuilt homes and a hospital to survive #COVID19 https://t.co/PLXqKc5K5d                                  \n",
       "5   Actionables for a healthy recovery from #COVID19 #climate https://t.co/crGlKZOz5Z                                                              \n",
       "6   Volume for those at the back please. üîä #COVID19 https://t.co/d0pn2Bv2Hx                                                                        \n",
       "7   Why has Ruto not eulogisied Mkapa!! Asking for Moses Kuria Raila #RutoSwindlingGEMA #COVID19                                                   \n",
       "8   Crazy that the world has come to this but as Americans we will fight to get through this!üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏ #covid19                                   \n",
       "9   @jimsciutto @JoAnnBaldwin55 People whose relatives have died from #COVID19 should file a class action lawsuit against Sinclair Broadcasting.   \n",
       "10  I miss isopropyl alcohol so much!!!! Ethanol in hand sanitizer smells like I poured tequila on my hands ü§¢ #COVID19                             \n",
       "11  @SonuSood sir mom is in ICU due to COVID just want prayers from you and everyone who is listening you . #COVID19                               \n",
       "12  While you at it, please follow me https://t.co/KheirQEh6Z #Fergusons #DurbanJuly #alreadyvideo #COVID19 #WiseUp https://t.co/h1TYdOgjwv        \n",
       "13  2,000 women lawyers write to #AmitShah seeking 5 Lakh Loan per financially drained lawyer. #COVID19 #Coronavirus https://t.co/JEjVTeCjEn       \n",
       "14  let us give it a try #alreadyvideo #COVID19 #HurricaneHanna https://t.co/2HChuAfNHy                                                            \n",
       "15  The Egyptians are not got shit on this. #COVID19 https://t.co/kl5lzOJaxO                                                                       \n",
       "16  Not only is the area about to be hit by #hanna a #COVID19 hotspot, but Hurricane Harvey work is ongoing. Still.                                \n",
       "17  1.28% of the you.S. population is infected with Covid-19 #COVID19 #TrumpVirus #MaskItOrCasket                                                  \n",
       "18  .@headout with a dashboard to boost post-#COVID19 #travel https://t.co/0gnRgqvLlh                                                              \n",
       "19  Highest ever number of new cases yesterday #coronavirus #covid19 #STAYatHOME https://t.co/oovQrvMviY                                           \n",
       "\n",
       "                                                                                  text_processed  \\\n",
       "0   [pray, good, health, recovery]                                                                 \n",
       "1   [july, update, discharge, people, test, actice, case]                                          \n",
       "2   [second, wave, flanders, back]                                                                 \n",
       "3   [holy, water, time]                                                                            \n",
       "4   [acknowledge, lack, rebuild, home, hospital, survive]                                          \n",
       "5   [actionables, healthy, recovery]                                                               \n",
       "6   [volume, back, please]                                                                         \n",
       "7   [ruto, eulogisied, mkapa, ask, moses, kuria, raila]                                            \n",
       "8   [crazy, world, come, american, fight, get]                                                     \n",
       "9   [people, whose, relative, die, file, class, action, lawsuit, sinclair, broadcasting]           \n",
       "10  [miss, isopropyl, alcohol, much, ethanol, hand, sanitizer, smell, like, pour, tequila, hand]   \n",
       "11  [sir, mom, icu, due, covid, want, prayer, everyone, listen]                                    \n",
       "12  [please, follow]                                                                               \n",
       "13  [woman, lawyer, write, seek, lakh, loan, per, financially, drain, lawyer]                      \n",
       "14  [let, u, give, try]                                                                            \n",
       "15  [egyptian, get, shit]                                                                          \n",
       "16  [area, hit, hotspot, hurricane, harvey, work, ongoing, still]                                  \n",
       "17  [population, infect, covid]                                                                    \n",
       "18  [dashboard, boost, post]                                                                       \n",
       "19  [high, ever, number, new, case, yesterday]                                                     \n",
       "\n",
       "                                                                                                                text_bigrams  \n",
       "0   [[good, health]]                                                                                                          \n",
       "1   [[people, test], [actice, case]]                                                                                          \n",
       "2   [[second, wave]]                                                                                                          \n",
       "3   [[holy, water]]                                                                                                           \n",
       "4   [[lack, rebuild], [rebuild, home]]                                                                                        \n",
       "5   [[healthy, recovery]]                                                                                                     \n",
       "6   [[back, please]]                                                                                                          \n",
       "7   [[eulogisied, mkapa], [moses, kuria], [kuria, raila]]                                                                     \n",
       "8   []                                                                                                                        \n",
       "9   [[people, whose], [whose, relative], [class, action], [action, lawsuit], [sinclair, broadcasting]]                        \n",
       "10  [[miss, isopropyl], [isopropyl, alcohol], [hand, sanitizer], [sanitizer, smell], [smell, like], [pour, tequila]]          \n",
       "11  [[sir, mom], [icu, due], [want, prayer]]                                                                                  \n",
       "12  [[please, follow]]                                                                                                        \n",
       "13  [[woman, lawyer], [lawyer, write], [lakh, loan], [loan, per], [per, financially], [financially, drain], [drain, lawyer]]  \n",
       "14  [[let, u], [u, give]]                                                                                                     \n",
       "15  [[get, shit]]                                                                                                             \n",
       "16  [[hurricane, harvey], [harvey, work]]                                                                                     \n",
       "17  []                                                                                                                        \n",
       "18  [[boost, post]]                                                                                                           \n",
       "19  [[high, ever], [ever, number], [new, case], [case, yesterday]]                                                            "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizes tweets using the default options.\n",
    "# dataset[\"text_processed\"] = [tokenize_normalize(text) for text in dataset[\"text\"].values.tolist()]\n",
    "# dataset[[\"text\", \"text_processed\"]].head(n=20)\n",
    "\n",
    "# Normalizes tweets and retrieves bigrams as well:\n",
    "dataset[\"text_processed\"], dataset[\"text_bigrams\"] = map(list, zip(*[tokenize_normalize(text, return_bigrams=True) for text in dataset[\"text\"].values.tolist()]))\n",
    "dataset[[\"text\", \"text_processed\", \"text_bigrams\"]].head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. There is still room for some improvements. For example, we could use [chunking](https://www.nltk.org/book/ch07.html#chunking) in order to combine linked tokens that constitute a single entity. For example, \"the,\" \"United,\" and \"States\" tokens could be combined as \"the United States.\" You may have noticed that the United States is commonly reffered as \"the U.S.\" and it causes residual-like tokens. We could create a dictionary to map these common abbreviations to their normal forms before tokenization (or add it to the expansions of contractions using `contractions.add(\"U.S.\" \"United States\")` so that it is corrected with the other contractions. Lastly, while our tokenizer shortens consecutively repeating characters to at most three consecutive repetitions (\"heyyyyyy!!!!!!!\" becomes \"heyyy!!!\") due to having `reduce_len=True`, it cannot handle \"üòÇüòÇüòÇüòÇüòÇ\" since they are separated into individual tokens. These token-level repetitions could be handled and consecutively repeated emojis could be even cleaned as well since they can affect n-gram distributions.\n",
    "\n",
    "Anyway, we can now look at the term frequencies. Top 50 frequent terms are listed below. We can later visualize them using plots or word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case           4590\n",
       "new            4004\n",
       "covid          3583\n",
       "death          2435\n",
       "test           2237\n",
       "get            1577\n",
       "mask           1435\n",
       "people         1359\n",
       "report         1318\n",
       "go             1282\n",
       "coronavirus    1280\n",
       "u              1263\n",
       "positive       1223\n",
       "pandemic       1138\n",
       "say            1114\n",
       "day            1102\n",
       "via            1061\n",
       "vaccine        934 \n",
       "time           879 \n",
       "update         830 \n",
       "one            805 \n",
       "like           793 \n",
       "take           770 \n",
       "die            761 \n",
       "good           760 \n",
       "news           748 \n",
       "need           747 \n",
       "wear           735 \n",
       "make           699 \n",
       "today          672 \n",
       "first          661 \n",
       "know           657 \n",
       "gmt            646 \n",
       "school         618 \n",
       "india          611 \n",
       "total          610 \n",
       "see            608 \n",
       "health         580 \n",
       "home           566 \n",
       "world          564 \n",
       "spread         559 \n",
       "work           557 \n",
       "virus          548 \n",
       "last           547 \n",
       "state          546 \n",
       "back           542 \n",
       "late           541 \n",
       "think          528 \n",
       "help           513 \n",
       "august         507 \n",
       "Name: text_processed, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flattening the lists together and counting the values:\n",
    "terms = pd.Series(dataset.explode('text_processed').text_processed).value_counts()\n",
    "\n",
    "terms.head(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the top 50 most frequent bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new - case               1179\n",
       "test - positive          650 \n",
       "new - death              564 \n",
       "new - covid              282 \n",
       "active - case            250 \n",
       "let - u                  247 \n",
       "death - toll             232 \n",
       "positive - case          193 \n",
       "face - mask              185 \n",
       "novel - coronavirus      156 \n",
       "stay - safe              154 \n",
       "test - negative          149 \n",
       "wear - mask              143 \n",
       "new - zealand            142 \n",
       "social - distancing      134 \n",
       "coronavirus - disease    128 \n",
       "coronavirus - case       128 \n",
       "total - case             125 \n",
       "look - like              115 \n",
       "south - africa           112 \n",
       "case - rise              107 \n",
       "case - report            101 \n",
       "every - american         100 \n",
       "confirm - case           100 \n",
       "district - report        97  \n",
       "united - state           96  \n",
       "home - minister          88  \n",
       "recovery - rate          88  \n",
       "good - news              86  \n",
       "first - time             78  \n",
       "stay - home              72  \n",
       "go - back                71  \n",
       "amit - shah              70  \n",
       "pm - edt                 68  \n",
       "second - wave            68  \n",
       "new - york               67  \n",
       "case - along             65  \n",
       "get - well               65  \n",
       "public - health          63  \n",
       "community - page         63  \n",
       "chief - minister         61  \n",
       "speedy - recovery        60  \n",
       "confirmed - case         59  \n",
       "case - today             59  \n",
       "new - coronavirus        59  \n",
       "tally - cross            58  \n",
       "county - covid           56  \n",
       "hong - kong              56  \n",
       "coronavirus - vaccine    55  \n",
       "death - report           55  \n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flattening the lists together, joining bigram list elements, and counting the values:\n",
    "bigrams = pd.Series([bigram[0]+\" - \"+bigram[1] for bigram in dataset.explode(\"text_bigrams\").text_bigrams.to_list() if type(bigram) == list]).value_counts().head(n=50)\n",
    "\n",
    "bigrams.head(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some approaches that look for n-unit word groups within a larger window (> 2), sentence, or short context such as tweet. They are computationally more expensive, but they can work better in certain situations. Another approach would be looking at the syntactic dependencies between words in the sentence. This can be helpful, because it does not rely on unit positions, it can flexibly detect word groups, and it does not create noise by pairing consecutive yet unrelated units. We will not cover dependency parsing here, but you can check [this page](https://en.wikipedia.org/wiki/Dependency_grammar) to learn more about it.\n",
    "\n",
    "Let us recap the processes we combined using a diagram:\n",
    "\n",
    "![NLP processes](nlp_processes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis<a id=\"sentiment\"></a>\n",
    "\n",
    "We can automatically detect the sentiment of words (and therefore sentences) without the need of manually reading labeling all the data. This can be especially useful for contents that target specific products or people, but we can apply this to our dataset as well. Sentiment analysis can be done using  We will simply use NLTK's sentiment analyzer that uses [VADER tool and lexicon](https://www.semanticscholar.org/paper/VADER%3A-A-Parsimonious-Rule-Based-Model-for-Analysis-Hutto-Gilbert/bcdc102c04fb0e7d4652e8bcc7edd2983bb9576d). Lexicons are collections of words designed for specific purposes and making sense of textual data in general. They can be about sentiments, emotions, or other purpose-specific words (plants, animals, etc.). We could collect positive words (such as \"nice\" and \"wonderful\") in a file, and check if a sentence has words from that file, indicating whether they are associated with positive sentiments. This would be our positive words lexicon. Alternatively, we could collect words that have a sentiment value (valence), and assign sentiment scores for them. So, we could assign \"great\" to \"1\" (positive) and \"terrible\" to \"-1\" (negative). VADER actually works like this. It is a word list that is manually rated by different individuals to obtain mean sentiment scores for each word. Naturally, there are other things to consider as well. For example, apart from negation, the existence of certain words also modify the sentiment's intensity (\"bad\" becomes even more negative with the word \"very\"). VADER handles these (and more) for us, so we can simply tokenize sentences and feed them to the analyzer to retrieve the normalized (between -1 and 1) sentence-level sentiment scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks VERY BAD!!! {'neg': 0.672, 'neu': 0.328, 'pos': 0.0, 'compound': -0.7984}\n",
      "It looks VERY BAD. {'neg': 0.637, 'neu': 0.363, 'pos': 0.0, 'compound': -0.7398}\n",
      "It looks very bad. {'neg': 0.558, 'neu': 0.442, 'pos': 0.0, 'compound': -0.5849}\n",
      "It looks bad. {'neg': 0.636, 'neu': 0.364, 'pos': 0.0, 'compound': -0.5423}\n",
      "It does not look very bad. {'neg': 0.0, 'neu': 0.62, 'pos': 0.38, 'compound': 0.4708}\n",
      "It does not look bad. {'neg': 0.0, 'neu': 0.584, 'pos': 0.416, 'compound': 0.431}\n",
      "It looks good. {'neg': 0.0, 'neu': 0.408, 'pos': 0.592, 'compound': 0.4404}\n",
      "It looks good! {'neg': 0.0, 'neu': 0.385, 'pos': 0.615, 'compound': 0.4926}\n",
      "It looks VERY BAD!!! It looks good! {'neg': 0.449, 'neu': 0.349, 'pos': 0.202, 'compound': -0.6733}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Notice how sentiment scores differ for the following sentences:\n",
    "sentences = [\"It looks VERY BAD!!!\", \n",
    "             \"It looks VERY BAD.\", \n",
    "             \"It looks very bad.\", \n",
    "             \"It looks bad.\", \n",
    "             \"It does not look very bad.\", \n",
    "             \"It does not look bad.\", \n",
    "             \"It looks good.\", \n",
    "             \"It looks good!\", \n",
    "             \"It looks VERY BAD!!! It looks good!\"]\n",
    "\n",
    "for sentence in sentences:\n",
    "    # It retrieves separate scores for negativity, neutrality, and positivity, but we \n",
    "    # can generally simply use the compound score as well.\n",
    "    print(sentence,sentiment_analyzer.polarity_scores(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is decent especially considering its ease of use. Note that the last example does not work so well, because we did not separate those two sentences.\n",
    "\n",
    "We can write a function that separates the sentences, computes their sentiment scores, and averages those scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1529"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function optionally takes tokenizer and analyzer objects. This is not needed, \n",
    "# but it can help with portability.\n",
    "def analyze_sentiment(tweet, sentence_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle'), \n",
    "                      sentiment_analyzer=SentimentIntensityAnalyzer()):\n",
    "    \n",
    "    # Sentences are tokenized:\n",
    "    sentences = sentence_tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Remember that tweets have hashtags, URLs, etc. that may confuse sentiment \n",
    "    # analysis process and pull the sentiment towards the middle. A sentiment score \n",
    "    # is not retrieved if the sentence has no sentiment and it has less than four\n",
    "    # tokens (including punctuation). So that a URL would not affect the sentiment \n",
    "    # while genuinely neutral sentences, given that they are long enough, would not\n",
    "    # be disregarded. Some of the processes we have used (such as URL removal) could\n",
    "    # be applied here as well. Feel free to play with its logic.\n",
    "    sentiments = [sentiment_analyzer.polarity_scores(sentence)[\"compound\"] for sentence in sentences \n",
    "                  if sentiment_analyzer.polarity_scores(sentence)[\"compound\"] != 0 or len(tokenizer.tokenize(sentence)) > 3]\n",
    "    \n",
    "    # Returns the average compound sentiment score if a sentiment is detected.\n",
    "    if sentiments:\n",
    "        return sum(sentiments)/len(sentiments)\n",
    "    # Returns 0 if a sentiment is not detected:\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# One sentence is negative and the other is positive while the general tone is closer\n",
    "# to being negative:\n",
    "analyze_sentiment(\"It looks VERY BAD!!! It looks good!\", sentence_tokenizer, sentiment_analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better. We can now apply this function the dataset and obtain the average sentiment score of each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Praying for good health and recovery of @ChouhanShivraj . #covid19 #covidPositive</td>\n",
       "      <td>0.65970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>July 25 #COVID19 update #TamilNadu - 6988 Discharge- 7758 People tested - 61729 Actice cases - 52273 #chennai - 1329</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Second wave of #COVID19 in Flanders..back to more #homework again... https://t.co/9swImZACMN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Holy water in times of #COVID19 https://t.co/YaZ49yxL27</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#FEMA acknowledges #PuertoRico lacks rebuilt homes and a hospital to survive #COVID19 https://t.co/PLXqKc5K5d</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Actionables for a healthy recovery from #COVID19 #climate https://t.co/crGlKZOz5Z</td>\n",
       "      <td>0.40190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Volume for those at the back please. üîä #COVID19 https://t.co/d0pn2Bv2Hx</td>\n",
       "      <td>0.31820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Why has Ruto not eulogisied Mkapa!! Asking for Moses Kuria Raila #RutoSwindlingGEMA #COVID19</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Crazy that the world has come to this but as Americans we will fight to get through this!üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏ #covid19</td>\n",
       "      <td>-0.65880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@jimsciutto @JoAnnBaldwin55 People whose relatives have died from #COVID19 should file a class action lawsuit against Sinclair Broadcasting.</td>\n",
       "      <td>-0.67050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I miss isopropyl alcohol so much!!!! Ethanol in hand sanitizer smells like I poured tequila on my hands ü§¢ #COVID19</td>\n",
       "      <td>0.13775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@SonuSood sir mom is in ICU due to COVID just want prayers from you and everyone who is listening you . #COVID19</td>\n",
       "      <td>0.07720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>While you at it, please follow me https://t.co/KheirQEh6Z #Fergusons #DurbanJuly #alreadyvideo #COVID19 #WiseUp https://t.co/h1TYdOgjwv</td>\n",
       "      <td>0.31820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2,000 women lawyers write to #AmitShah seeking 5 Lakh Loan per financially drained lawyer. #COVID19 #Coronavirus https://t.co/JEjVTeCjEn</td>\n",
       "      <td>-0.36120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>let us give it a try #alreadyvideo #COVID19 #HurricaneHanna https://t.co/2HChuAfNHy</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Egyptians are not got shit on this. #COVID19 https://t.co/kl5lzOJaxO</td>\n",
       "      <td>0.44490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Not only is the area about to be hit by #hanna a #COVID19 hotspot, but Hurricane Harvey work is ongoing. Still.</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.28% of the you.S. population is infected with Covid-19 #COVID19 #TrumpVirus #MaskItOrCasket</td>\n",
       "      <td>-0.24695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.@headout with a dashboard to boost post-#COVID19 #travel https://t.co/0gnRgqvLlh</td>\n",
       "      <td>0.40190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Highest ever number of new cases yesterday #coronavirus #covid19 #STAYatHOME https://t.co/oovQrvMviY</td>\n",
       "      <td>0.07720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@bubbaprog 350 school employees exposed to #COVID19 in Manatee County schools over the summer https://t.co/GxiYb61ay0</td>\n",
       "      <td>-0.07720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@BenJamesPhotos Italy #BenPC #LOCKDOWNPHOTOCHALLENGE #COVID19 https://t.co/27jyDBhbMc</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>If CM of Madhya Pradesh is tested positive for COVID, then Scindia should also be tested for #COVID19.</td>\n",
       "      <td>0.55740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Bihar witnesses biggest single-day spike of 2,803 new #Covid19 cases https://t.co/zTPksALE7T https://t.co/979SPKzt3w</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Lunch in #Amsterdam away from the mass tourists that have invaded so quickly during the #COVID19 pandemic ! #travel https://t.co/DBCPWA0Ao9</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            text  \\\n",
       "0   Praying for good health and recovery of @ChouhanShivraj . #covid19 #covidPositive                                                              \n",
       "1   July 25 #COVID19 update #TamilNadu - 6988 Discharge- 7758 People tested - 61729 Actice cases - 52273 #chennai - 1329                           \n",
       "2   Second wave of #COVID19 in Flanders..back to more #homework again... https://t.co/9swImZACMN                                                   \n",
       "3   Holy water in times of #COVID19 https://t.co/YaZ49yxL27                                                                                        \n",
       "4   #FEMA acknowledges #PuertoRico lacks rebuilt homes and a hospital to survive #COVID19 https://t.co/PLXqKc5K5d                                  \n",
       "5   Actionables for a healthy recovery from #COVID19 #climate https://t.co/crGlKZOz5Z                                                              \n",
       "6   Volume for those at the back please. üîä #COVID19 https://t.co/d0pn2Bv2Hx                                                                        \n",
       "7   Why has Ruto not eulogisied Mkapa!! Asking for Moses Kuria Raila #RutoSwindlingGEMA #COVID19                                                   \n",
       "8   Crazy that the world has come to this but as Americans we will fight to get through this!üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏üá∫üá∏ #covid19                                   \n",
       "9   @jimsciutto @JoAnnBaldwin55 People whose relatives have died from #COVID19 should file a class action lawsuit against Sinclair Broadcasting.   \n",
       "10  I miss isopropyl alcohol so much!!!! Ethanol in hand sanitizer smells like I poured tequila on my hands ü§¢ #COVID19                             \n",
       "11  @SonuSood sir mom is in ICU due to COVID just want prayers from you and everyone who is listening you . #COVID19                               \n",
       "12  While you at it, please follow me https://t.co/KheirQEh6Z #Fergusons #DurbanJuly #alreadyvideo #COVID19 #WiseUp https://t.co/h1TYdOgjwv        \n",
       "13  2,000 women lawyers write to #AmitShah seeking 5 Lakh Loan per financially drained lawyer. #COVID19 #Coronavirus https://t.co/JEjVTeCjEn       \n",
       "14  let us give it a try #alreadyvideo #COVID19 #HurricaneHanna https://t.co/2HChuAfNHy                                                            \n",
       "15  The Egyptians are not got shit on this. #COVID19 https://t.co/kl5lzOJaxO                                                                       \n",
       "16  Not only is the area about to be hit by #hanna a #COVID19 hotspot, but Hurricane Harvey work is ongoing. Still.                                \n",
       "17  1.28% of the you.S. population is infected with Covid-19 #COVID19 #TrumpVirus #MaskItOrCasket                                                  \n",
       "18  .@headout with a dashboard to boost post-#COVID19 #travel https://t.co/0gnRgqvLlh                                                              \n",
       "19  Highest ever number of new cases yesterday #coronavirus #covid19 #STAYatHOME https://t.co/oovQrvMviY                                           \n",
       "20  @bubbaprog 350 school employees exposed to #COVID19 in Manatee County schools over the summer https://t.co/GxiYb61ay0                          \n",
       "21  @BenJamesPhotos Italy #BenPC #LOCKDOWNPHOTOCHALLENGE #COVID19 https://t.co/27jyDBhbMc                                                          \n",
       "22  If CM of Madhya Pradesh is tested positive for COVID, then Scindia should also be tested for #COVID19.                                         \n",
       "23  Bihar witnesses biggest single-day spike of 2,803 new #Covid19 cases https://t.co/zTPksALE7T https://t.co/979SPKzt3w                           \n",
       "24  Lunch in #Amsterdam away from the mass tourists that have invaded so quickly during the #COVID19 pandemic ! #travel https://t.co/DBCPWA0Ao9    \n",
       "\n",
       "    sentiment  \n",
       "0   0.65970    \n",
       "1   0.00000    \n",
       "2   0.00000    \n",
       "3   0.00000    \n",
       "4   0.00000    \n",
       "5   0.40190    \n",
       "6   0.31820    \n",
       "7   0.00000    \n",
       "8  -0.65880    \n",
       "9  -0.67050    \n",
       "10  0.13775    \n",
       "11  0.07720    \n",
       "12  0.31820    \n",
       "13 -0.36120    \n",
       "14  0.00000    \n",
       "15  0.44490    \n",
       "16  0.00000    \n",
       "17 -0.24695    \n",
       "18  0.40190    \n",
       "19  0.07720    \n",
       "20 -0.07720    \n",
       "21  0.00000    \n",
       "22  0.55740    \n",
       "23  0.00000    \n",
       "24  0.00000    "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"sentiment\"] = dataset[\"text\"].map(analyze_sentiment)\n",
    "\n",
    "dataset[[\"text\", \"sentiment\"]].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not perfect as you can see, but it is much more reliable at the extremes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34980</th>\n",
       "      <td>@OfficialRedSide @MSNBC Just the fake president who raped women is a racist and does not care about the people that died of #COVID19</td>\n",
       "      <td>-0.9579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20590</th>\n",
       "      <td>Failure after failure, lie after lie by this stinking, rotten corrupt shower of shit we have for a Govt. #COVID19 https://t.co/IhkenBP2Hg</td>\n",
       "      <td>-0.9552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35399</th>\n",
       "      <td>Violence and abuse against shop workers doubles during #COVID19 pandemic - wtf is wrong with people!! https://t.co/xBckn6IBNq</td>\n",
       "      <td>-0.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31704</th>\n",
       "      <td>https://t.co/m3B4eKyGTg Sighhhhhhhhhhhhhhhhhhh :( :( :( :( :( :( #covid19</td>\n",
       "      <td>-0.9468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>@TheDailyEdge @seanhannity @FoxNews So he can spread lies, hate, sexual violence, and murder. #COVID19 #FoxNews</td>\n",
       "      <td>-0.9460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4809</th>\n",
       "      <td>#COVID19 has really become boring o, next year we want walking dead, evil dead, werewolves and vampires ü§£ü§£ü§£ü§£ #BBNaijia2020</td>\n",
       "      <td>-0.9458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>Not so fun fact: More Americans have died from #Covid19 than were killed during the entire Vietnam war. #CoronaVirus #CoronaVirusPandemic</td>\n",
       "      <td>-0.9449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14787</th>\n",
       "      <td>@matthaig1 Also it is friggin stupid because from all accounts #COVID19 is a horrible scary drawn-out way to die.</td>\n",
       "      <td>-0.9359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9525</th>\n",
       "      <td>#COVID19 and Suicide: A Crisis Within a Crisis #MentalHealthMonday #MentalHealthMatters #MentalHealth https://t.co/XbHu4srCLF</td>\n",
       "      <td>-0.9287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38477</th>\n",
       "      <td>@scott_ipod 182,000 DEAD AMERICANS MURDERED BY #TrumpsAmerica due to ignorance in a #COVID19 pandemic = 62 x 9/11's</td>\n",
       "      <td>-0.9283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25145</th>\n",
       "      <td>@EricEdmeades I am sure your immune system is bulletproof Eric, if not there is no hope for the rest of us #COVID19 good luck üëç</td>\n",
       "      <td>0.9215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29303</th>\n",
       "      <td>Had a wonderful curbside #COVID19 #birthday thank you all for your blessings and cheer! https://t.co/G1UgjPAmma</td>\n",
       "      <td>0.9230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>So The Lord is so good He can love discipline forgive and set us free at the same time. #COVID19 #BLM @justiceDriver1</td>\n",
       "      <td>0.9237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8012</th>\n",
       "      <td>Congratulations to @TytoCare for winning the #Covid19 Innovation Award by @ExTechChallenge üèÜü•≥ https://t.co/PjVxdSHsMH</td>\n",
       "      <td>0.9246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7503</th>\n",
       "      <td>It is clear that our favorite furry friend will once again help save us: https://t.co/gMkxN9xFxx #COVID19</td>\n",
       "      <td>0.9287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19047</th>\n",
       "      <td>@BreesAnna Great interview Anna and such a pleasant change to hear someone talking sensible about #COVID19 Hope you have wonderful weekend</td>\n",
       "      <td>0.9325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35358</th>\n",
       "      <td>Great to see the growth of #virtual chapter events as ways of engaging during #COVID19 good luck! https://t.co/xsnXA1J6hw</td>\n",
       "      <td>0.9359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35353</th>\n",
       "      <td>Great to see the growth of #virtual chapter events as ways of engaging during #COVID19 good luck! https://t.co/nvYV7o7MFt</td>\n",
       "      <td>0.9359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23488</th>\n",
       "      <td>I have booked 4 sessions - super value and super talented smart #women in the time of #covid19 https://t.co/uexC6DcdZd</td>\n",
       "      <td>0.9451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33212</th>\n",
       "      <td>Extremely worthwhile article üëá with super valuable info that can help you or a loved one survive #COVID19 üëáüëáüëá https://t.co/giOxdQSZZ0</td>\n",
       "      <td>0.9459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             text  \\\n",
       "34980  @OfficialRedSide @MSNBC Just the fake president who raped women is a racist and does not care about the people that died of #COVID19         \n",
       "20590  Failure after failure, lie after lie by this stinking, rotten corrupt shower of shit we have for a Govt. #COVID19 https://t.co/IhkenBP2Hg    \n",
       "35399  Violence and abuse against shop workers doubles during #COVID19 pandemic - wtf is wrong with people!! https://t.co/xBckn6IBNq                \n",
       "31704  https://t.co/m3B4eKyGTg Sighhhhhhhhhhhhhhhhhhh :( :( :( :( :( :( #covid19                                                                    \n",
       "36     @TheDailyEdge @seanhannity @FoxNews So he can spread lies, hate, sexual violence, and murder. #COVID19 #FoxNews                              \n",
       "4809   #COVID19 has really become boring o, next year we want walking dead, evil dead, werewolves and vampires ü§£ü§£ü§£ü§£ #BBNaijia2020                   \n",
       "2225   Not so fun fact: More Americans have died from #Covid19 than were killed during the entire Vietnam war. #CoronaVirus #CoronaVirusPandemic    \n",
       "14787  @matthaig1 Also it is friggin stupid because from all accounts #COVID19 is a horrible scary drawn-out way to die.                            \n",
       "9525   #COVID19 and Suicide: A Crisis Within a Crisis #MentalHealthMonday #MentalHealthMatters #MentalHealth https://t.co/XbHu4srCLF                \n",
       "38477  @scott_ipod 182,000 DEAD AMERICANS MURDERED BY #TrumpsAmerica due to ignorance in a #COVID19 pandemic = 62 x 9/11's                          \n",
       "25145  @EricEdmeades I am sure your immune system is bulletproof Eric, if not there is no hope for the rest of us #COVID19 good luck üëç              \n",
       "29303  Had a wonderful curbside #COVID19 #birthday thank you all for your blessings and cheer! https://t.co/G1UgjPAmma                              \n",
       "358    So The Lord is so good He can love discipline forgive and set us free at the same time. #COVID19 #BLM @justiceDriver1                        \n",
       "8012   Congratulations to @TytoCare for winning the #Covid19 Innovation Award by @ExTechChallenge üèÜü•≥ https://t.co/PjVxdSHsMH                        \n",
       "7503   It is clear that our favorite furry friend will once again help save us: https://t.co/gMkxN9xFxx #COVID19                                    \n",
       "19047  @BreesAnna Great interview Anna and such a pleasant change to hear someone talking sensible about #COVID19 Hope you have wonderful weekend   \n",
       "35358  Great to see the growth of #virtual chapter events as ways of engaging during #COVID19 good luck! https://t.co/xsnXA1J6hw                    \n",
       "35353  Great to see the growth of #virtual chapter events as ways of engaging during #COVID19 good luck! https://t.co/nvYV7o7MFt                    \n",
       "23488  I have booked 4 sessions - super value and super talented smart #women in the time of #covid19 https://t.co/uexC6DcdZd                       \n",
       "33212  Extremely worthwhile article üëá with super valuable info that can help you or a loved one survive #COVID19 üëáüëáüëá https://t.co/giOxdQSZZ0        \n",
       "\n",
       "       sentiment  \n",
       "34980 -0.9579     \n",
       "20590 -0.9552     \n",
       "35399 -0.9500     \n",
       "31704 -0.9468     \n",
       "36    -0.9460     \n",
       "4809  -0.9458     \n",
       "2225  -0.9449     \n",
       "14787 -0.9359     \n",
       "9525  -0.9287     \n",
       "38477 -0.9283     \n",
       "25145  0.9215     \n",
       "29303  0.9230     \n",
       "358    0.9237     \n",
       "8012   0.9246     \n",
       "7503   0.9287     \n",
       "19047  0.9325     \n",
       "35358  0.9359     \n",
       "35353  0.9359     \n",
       "23488  0.9451     \n",
       "33212  0.9459     "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combines the most positive and negative 10 tweets together:\n",
    "dataset[[\"text\", \"sentiment\"]].sort_values(by=['sentiment']).head(10).append(dataset[[\"text\", \"sentiment\"]].sort_values(by=['sentiment']).tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these sentiment scores with other analyses. You are encouraged to explore these tweets and other methods for analysis.\n",
    "\n",
    "Note that there are some important limitations. A word can have different meanings in different domains/contexts. Consider these sentences:\n",
    "\n",
    "* This song is **sick**. ‚Üí Nice.üëç\n",
    "* This child is **sick**. ‚Üí Not nice.üëé\n",
    "\n",
    "Sometimes, a word can have one meaning yet different implications. Consider these sentences:\n",
    "\n",
    "* The patient had a **positive** experience. ‚Üí Nice.üëç\n",
    "* The patient had a **positive** test result for COVID-19. ‚Üí Not nice.üëé\n",
    "\n",
    "You might want to make adjustments or use your own lexicon for a specific topic/task. You can also train a model to guess the sentiment. [Check](https://towardsdatascience.com/basic-binary-sentiment-analysis-using-nltk-c94ba17ae386) this example.\n",
    "\n",
    "If you are interested in lexicon-based approaches, take a look at [EmoLex](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm), a lexicon for emotions and sentiment. It can be used to automatize emotion detection. It also has Turkish words (along with some other languages), but they are an automatic translation, so they are not as reliable. You can now directly download it from its web page.\n",
    "\n",
    "## Bonus: Levenshtein distance<a id=\"levenshtein\"></a>\n",
    "\n",
    "As mentioned above, a simple autocorrection process can be applied using the Levenshtein distance. Let us see what that means. \n",
    "\n",
    "Consider the words `cup` and `cap`. Their Levenshtein distance is 1, because we can obtain one from the other by simply substituting a character. For two given strings, we can calculate the number of operations required (edit distance) to obtain one from the another. These operations are:\n",
    "\n",
    "* Insertion: Inserting a character to a specific location in the string.\n",
    "    * \"up\" becomes \"**c**up\"\n",
    "* Deletion: Deleting a character from a specific location in the string.\n",
    "    * \"cu**s**p\" becomes \"cup\"\n",
    "* Substitution: Substituting a character in a specific location in the string with another character.\n",
    "    * \"c**u**p\" becomes \"c**a**p\"\n",
    "* Transpotisition (this is later introduced by an extension, the Damerau-Levenstein distance algorithm): Switching the positions of two adjacent characters.\n",
    "    * \"c**pu**\" becomes \"c**up**\"\n",
    "    \n",
    "Using these, for a given word, we can find the closest word from a dictionary that would require the least amount of changes. This is a costly process, so it is usually limited to certain amount of changes. See [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) for more information.\n",
    "\n",
    "We can also use this distance to calculate the similarity between two strings. This is handy for fuzzy string matching, when the same thing can be represented in similar yet different forms. This is quite common in neighborhood or street names in Turkey. By setting a similarity threshold and looking at their similarity, we can match addresses like `Kemalpa≈üa Mah.` and `Kemal Pa≈üa Mahallesi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7058823529411765"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "string_a = \"Kemalpa≈üa Mah.\"\n",
    "string_b = \"Kemal Pa≈üa Mahallesi\"\n",
    "\n",
    "Levenshtein.ratio(string_a, string_b)\n",
    "# Note that you would probably want to remove \"mahallesi\" or \"mah.\" when your task is \n",
    "# address matching. It would significantly increase your success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A use case for our dataset could be finding tweets that are similar to each other. Let us search for a tweet pair that has the highest similarity without being exactly the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar tweet pair: ('395 new cases and 3 new deaths in Uzbekistan [13:22 GMT] #coronavirus #CoronaVirusUpdate #COVID19 #CoronavirusPandemic', '1,005 new cases and 18 new deaths in the United States [13:16 GMT] #coronavirus #CoronaVirusUpdate #COVID19 #CoronavirusPandemic')\n",
      "Similarity 0.8699186991869918\n"
     ]
    }
   ],
   "source": [
    "pair = None\n",
    "highest = 0\n",
    "\n",
    "tweets_to_compare = 100\n",
    "# To compare all the tweets, uncomment this line. Note that it would take much longer.\n",
    "# tweets_to_compare = dataset.shape[0]\n",
    "\n",
    "# This compares each tweet with the ones that come after itself, which takes some time.\n",
    "for i in range(0, tweets_to_compare-1):\n",
    "    for j in range(1, tweets_to_compare):\n",
    "        # If the tweets are not the same:\n",
    "        if dataset.loc[i,\"text\"] != dataset.loc[j,\"text\"]:\n",
    "            similarity = Levenshtein.ratio(dataset.loc[i,\"text\"], dataset.loc[j,\"text\"])\n",
    "            # If their similarity is higher than the previous similarities:\n",
    "            if similarity > highest:\n",
    "                highest = similarity\n",
    "                pair = (dataset.loc[i,\"text\"], dataset.loc[j,\"text\"])\n",
    "                \n",
    "print(\"The most similar tweet pair:\", pair)\n",
    "print(\"Similarity\", highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: NLP in Turkish<a id=\"tr\"></a>\n",
    "\n",
    "From a linguistic perspective, Turkish is a fascinating language with its rather strict grammatical rules. However, due to its agglutinative nature, words can easily become complex with many affixes and inflections. This can make morphological analyses harder compared to English.\n",
    "\n",
    "### Stemming<a id=\"tr-stem\"></a>\n",
    "\n",
    "[snowballstemmer](https://pypi.org/project/snowballstemmer/) has a Turkish stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g√∂zle', 'g√∂zleye', 'g√∂zl√ºkl√º', 'g√∂zc√º', 'g√∂z', 'd√º≈ü', '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snowballstemmer import TurkishStemmer\n",
    "\n",
    "stemmer_tr = TurkishStemmer()\n",
    "\n",
    "sentence = \"G√∂zleme g√∂zleyen g√∂zl√ºkl√º g√∂zc√º g√∂zden d√º≈üt√º.\"\n",
    "\n",
    "[stemmer_tr.stemWord(token) for token in tokenizer.tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging and lemmatization<a id=\"tr-pos-lemma\"></a>\n",
    "\n",
    "It looks like the morphological analyzer of [Zemberek](https://github.com/ahmetaa/zemberek-nlp), the famous Turkish NLP tool for Java, has been unofficially ported to Python as [zeyrek](https://github.com/obulat/zeyrek/). It does not have all of its original features (like disambiguation and more), but we can still use it to morphologically analyze a word, sentence, or sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Parse(word='G√∂zleme', lemma='g√∂zlemek', pos='Verb', morphemes=['Verb', 'Neg', 'Imp', 'A2sg'], formatted='[g√∂zlemek:Verb] g√∂zle:Verb+me:Neg+Imp+A2sg'),\n",
       "  Parse(word='G√∂zleme', lemma='G√∂zlem', pos='Noun', morphemes=['Noun', 'A3sg', 'Dat'], formatted='[G√∂zlem:Noun,Prop] g√∂zlem:Noun+A3sg+e:Dat'),\n",
       "  Parse(word='G√∂zleme', lemma='g√∂zlem', pos='Noun', morphemes=['Noun', 'A3sg', 'Dat'], formatted='[g√∂zlem:Noun] g√∂zlem:Noun+A3sg+e:Dat'),\n",
       "  Parse(word='G√∂zleme', lemma='g√∂zleme', pos='Noun', morphemes=['Noun', 'A3sg'], formatted='[g√∂zleme:Noun] g√∂zleme:Noun+A3sg'),\n",
       "  Parse(word='G√∂zleme', lemma='g√∂zlemek', pos='Noun', morphemes=['Verb', 'Inf2', 'Noun', 'A3sg'], formatted='[g√∂zlemek:Verb] g√∂zle:Verb|me:Inf2‚ÜíNoun+A3sg')],\n",
       " [Parse(word='g√∂zleyen', lemma='g√∂zlemek', pos='Adj', morphemes=['Verb', 'PresPart', 'Adj'], formatted='[g√∂zlemek:Verb] g√∂zle:Verb|yen:PresPart‚ÜíAdj')],\n",
       " [Parse(word='g√∂zl√ºkl√º', lemma='g√∂zl√ºk', pos='Adj', morphemes=['Noun', 'A3sg', 'With', 'Adj'], formatted='[g√∂zl√ºk:Noun] g√∂zl√ºk:Noun+A3sg|l√º:With‚ÜíAdj'),\n",
       "  Parse(word='g√∂zl√ºkl√º', lemma='g√∂z', pos='Adj', morphemes=['Noun', 'A3sg', 'Ness', 'Noun', 'A3sg', 'With', 'Adj'], formatted='[g√∂z:Noun] g√∂z:Noun+A3sg|l√ºk:Ness‚ÜíNoun+A3sg|l√º:With‚ÜíAdj')],\n",
       " [Parse(word='g√∂zc√º', lemma='g√∂zc√º', pos='Noun', morphemes=['Noun', 'A3sg'], formatted='[g√∂zc√º:Noun] g√∂zc√º:Noun+A3sg'),\n",
       "  Parse(word='g√∂zc√º', lemma='g√∂z', pos='Noun', morphemes=['Noun', 'A3sg', 'Agt', 'Noun', 'A3sg'], formatted='[g√∂z:Noun] g√∂z:Noun+A3sg|c√º:Agt‚ÜíNoun+A3sg')],\n",
       " [Parse(word='g√∂zden', lemma='g√∂z', pos='Noun', morphemes=['Noun', 'A3sg', 'Abl'], formatted='[g√∂z:Noun] g√∂z:Noun+A3sg+den:Abl'),\n",
       "  Parse(word='g√∂zden', lemma='G√∂zde', pos='Noun', morphemes=['Noun', 'A3sg', 'P2sg'], formatted='[G√∂zde:Noun,Prop] g√∂zde:Noun+A3sg+n:P2sg'),\n",
       "  Parse(word='g√∂zden', lemma='g√∂zde', pos='Noun', morphemes=['Noun', 'A3sg', 'P2sg'], formatted='[g√∂zde:Noun] g√∂zde:Noun+A3sg+n:P2sg'),\n",
       "  Parse(word='g√∂zden', lemma='g√∂zde', pos='Noun', morphemes=['Adj', 'Zero', 'Noun', 'A3sg', 'P2sg'], formatted='[g√∂zde:Adj] g√∂zde:Adj|Zero‚ÜíNoun+A3sg+n:P2sg')],\n",
       " [Parse(word='d√º≈üt√º', lemma='d√º≈ümek', pos='Verb', morphemes=['Verb', 'Past', 'A3sg'], formatted='[d√º≈ümek:Verb] d√º≈ü:Verb+t√º:Past+A3sg'),\n",
       "  Parse(word='d√º≈üt√º', lemma='d√º≈ü', pos='Verb', morphemes=['Noun', 'A3sg', 'Zero', 'Verb', 'Past', 'A3sg'], formatted='[d√º≈ü:Noun] d√º≈ü:Noun+A3sg|Zero‚ÜíVerb+t√º:Past+A3sg')],\n",
       " [Parse(word='.', lemma='.', pos='Punc', morphemes=['Punc'], formatted='[.:Punc] .:Punc')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zeyrek import MorphAnalyzer\n",
    "\n",
    "analyzer = MorphAnalyzer()\n",
    "\n",
    "analyzer.analyze(sentence)\n",
    "\n",
    "# This also explicitly returns tokenized sentences if you prefer:\n",
    "# analyzer._analyze_text(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some words can have different morphological explanations, every alternative is retrieved. From this, we can obtain lemmas and POS tags as you can see. However, simply using the first explanation for a word may not yield the correct result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: G√∂zleme \n",
      "POS: Verb \n",
      "Lemma: g√∂zlemek \n",
      "\n",
      "Token: g√∂zleyen \n",
      "POS: Adj \n",
      "Lemma: g√∂zlemek \n",
      "\n",
      "Token: g√∂zl√ºkl√º \n",
      "POS: Adj \n",
      "Lemma: g√∂zl√ºk \n",
      "\n",
      "Token: g√∂zc√º \n",
      "POS: Noun \n",
      "Lemma: g√∂zc√º \n",
      "\n",
      "Token: g√∂zden \n",
      "POS: Noun \n",
      "Lemma: g√∂z \n",
      "\n",
      "Token: d√º≈üt√º \n",
      "POS: Verb \n",
      "Lemma: d√º≈ümek \n",
      "\n",
      "Token: . \n",
      "POS: Punc \n",
      "Lemma: . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in analyzer.analyze(sentence):\n",
    "    print(\"Token:\",word[0].word,\"\\nPOS:\",word[0].pos,\"\\nLemma:\",word[0].lemma,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also simply use `analyzer.lemmatize()` to lemmatize the words. Again, it returns all possible lemmas without disambiguation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('G√∂zleme', ['g√∂zlemek', 'g√∂zleme', 'G√∂zlem', 'g√∂zlem']),\n",
       " ('g√∂zleyen', ['g√∂zlemek']),\n",
       " ('g√∂zl√ºkl√º', ['g√∂zl√ºk', 'g√∂z']),\n",
       " ('g√∂zc√º', ['g√∂zc√º', 'g√∂z']),\n",
       " ('g√∂zden', ['G√∂zde', 'g√∂zde', 'g√∂z']),\n",
       " ('d√º≈üt√º', ['d√º≈ümek', 'd√º≈ü']),\n",
       " ('.', ['.'])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.lemmatize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Turkish characters<a id=\"tr-encoding\"></a>\n",
    "\n",
    "A past Turkish government's short-sightedness in the 80's is haunting programmers who work with Turkish texts to this day. To prevent some Turkish characters from deforming, we need to read and write files using the \"UTF-8\" encoding. However, this may not be enough if your data is not saved as \"UTF-8\" in the first place. Sometimes, you may realize that certain characters in your dataset itself is not properly represented. For example, instead of the word `kƒ±lƒ±√ß`, you may see `k√Ñ¬±l√Ñ¬±√É¬ß`. This may suggest that your data is saved in \"Latin-1\" (also known as \"ISO 8859-1\"), which is the most common encoding in the Western world and the standard for many protocols. To fix it, you can specify the encoding of the string as \"latin-1\" (or \"ISO 8859-1\") and then decode it to \"UTF-8\" as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kƒ±lƒ±√ß'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badly_encoded = \"k√Ñ¬±l√Ñ¬±√É¬ß\"\n",
    "encoding_fixed = badly_encoded.encode(\"latin-1\").decode(\"utf-8\")\n",
    "\n",
    "encoding_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this will solve your problem. If not, you can also find the deformed characters and write a function that replaces all of those characters with the correct ones. \n",
    "\n",
    "Keep in mind that Turkish characters can also cause some packages or languages to raise an error. You may also see that the fonts used by some packages may not support these characters and plot `‚ñ°` instead. Therefore, you may want to anglicize Turkish characters to prevent these. Here is a function that does that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kilic'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changes all Turkish characters with their simpler versions:\n",
    "def anglicize_turkish(text):\n",
    "    return text.translate(text.maketrans({\"ƒû\": \"G\", \"ƒü\": \"g\", \"√ú\": \"U\", \"√º\": \"u\", \"≈û\": \"S\", \"≈ü\": \"s\", \"ƒ∞\": \"I\", \"ƒ±\": \"i\", \"√ñ\": \"O\", \"√∂\": \"o\", \"√á\": \"C\", \"√ß\": \"c\"}))\n",
    "\n",
    "anglicize_turkish(\"kƒ±lƒ±√ß\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem is the built-in functions that lowercase or uppercase your text may not work properly with Turkish text, even if your system locale is Turkish. For example, the lowercase version of `KILI√á` is normally returned as `kili√ß`, since \"I\" corresponds to \"i\" in the Western languages. Instead of dealing with locales (which may not solve your problem anyway), a simple solution is to manually lowercase/uppercase the problematic letters and then use the built-in function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built-in lowercase for KILI√á: kili√ß\n",
      "Custom lowercase for KILI√á: kƒ±lƒ±√ß\n",
      "Built-in uppercase for isim: ISIM\n",
      "Custom uppercase for isim: ƒ∞Sƒ∞M\n"
     ]
    }
   ],
   "source": [
    "def turkish_lowercase(text):\n",
    "    return text.translate(text.maketrans({\"I\": \"ƒ±\"})).lower()\n",
    "\n",
    "def turkish_uppercase(text):\n",
    "    return text.translate(text.maketrans({\"i\": \"ƒ∞\"})).upper()\n",
    "\n",
    "print(\"Built-in lowercase for KILI√á:\", \"KILI√á\".lower())\n",
    "print(\"Custom lowercase for KILI√á:\", turkish_lowercase(\"KILI√á\"))\n",
    "print(\"Built-in uppercase for isim:\", \"isim\".upper())\n",
    "print(\"Custom uppercase for isim:\", turkish_uppercase(\"isim\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information<a id=\"more-info\"></a>\n",
    "\n",
    "* [NLTK documentation](https://www.nltk.org/)\n",
    "* [TextBlob](https://textblob.readthedocs.io/en/dev/): A library written on NLTK. You may want to look at this as well since certain tasks can be easier with TextBlob.\n",
    "* [spaCy](https://github.com/explosion/spaCy): Another popular NLP library.\n",
    "* [Stanza](https://stanfordnlp.github.io/stanza/): An NLP library with a language-agnostic pipeline that makes use of neural networks. It also provides an interface for [CoreNLP](https://stanfordnlp.github.io/CoreNLP/), a popular NLP library for Java. It supports Turkish. You can check how it works with Turkish (or any other language) from [here](http://stanza.run/).\n",
    "* If you are interested in linguistics, you can take a look at [the course catalog of Cognitive Science Department at METU](https://catalog.metu.edu.tr/prog_courses.php?prog=902) as they have some specialized courses.\n",
    "* [Social media text normalization for Turkish](https://www.cambridge.org/core/journals/natural-language-engineering/article/social-media-text-normalization-for-turkish/6BADFEB835E28ABC03CDC472B2BAA6AB)\n",
    "\n",
    "If you have questions, feel free to send an e-mail."
   ]
  }
 ],
 "metadata": {
  "author": "√ñzg√ºn Ozan Kƒ±lƒ±√ß",
  "institute": "Informatics Institute, Middle East Technical University",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
